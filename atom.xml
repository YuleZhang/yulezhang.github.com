<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YuleZhang&#39;s Blogs</title>
  
  <subtitle>坚持走自己的路！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yulezhang.com/"/>
  <updated>2021-07-25T13:18:52.710Z</updated>
  <id>http://www.yulezhang.com/</id>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Vacation_Week3</title>
    <link href="http://www.yulezhang.com/2021/07/25/77SummerVacation_Week3/"/>
    <id>http://www.yulezhang.com/2021/07/25/77SummerVacation_Week3/</id>
    <published>2021-07-25T14:04:00.000Z</published>
    <updated>2021-07-25T13:18:52.710Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;本周学习了《Learning Relevant Features of Data with Multi-scale Tensor Networks》、《Isometric Tensor Network States in Two
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Vacation_Week1</title>
    <link href="http://www.yulezhang.com/2021/07/11/75SummerVacation_Week1/"/>
    <id>http://www.yulezhang.com/2021/07/11/75SummerVacation_Week1/</id>
    <published>2021-07-11T14:04:00.000Z</published>
    <updated>2021-07-12T02:44:30.938Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;本周是我们暑假特训的第一周，重点阅读了《Number-State Preserving Tensor Networks as Classifiers for Supervised Learning 》和《Generative tensor network
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Vacation_Week1</title>
    <link href="http://www.yulezhang.com/2021/07/11/76SummerVacation_Week2/"/>
    <id>http://www.yulezhang.com/2021/07/11/76SummerVacation_Week2/</id>
    <published>2021-07-11T14:04:00.000Z</published>
    <updated>2021-07-18T06:43:47.846Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;本周重点阅读了《Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz》(MERA)和《Tensor networks for unsupervised
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/06/20/73ExploreScheme_7/"/>
    <id>http://www.yulezhang.com/2021/06/20/73ExploreScheme_7/</id>
    <published>2021-06-20T14:05:00.000Z</published>
    <updated>2021-06-20T13:27:14.239Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;论文修改&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[x] 与AI的可解释性模型进行对比&lt;/li&gt;
&lt;li&gt;[x] 2分类3分类的生成的标签值&lt;/li&gt;
&lt;li&gt;[x] 假图片分类准确率计算&lt;/li&gt;
&lt;li&gt;[x]
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/06/20/74ExploreScheme_8/"/>
    <id>http://www.yulezhang.com/2021/06/20/74ExploreScheme_8/</id>
    <published>2021-06-20T14:05:00.000Z</published>
    <updated>2021-06-22T01:50:49.581Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;论文修改&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;also、certain、several不要用，which少出现，要简化&lt;/li&gt;
&lt;li&gt;The update algorithm of TNs is mainly related to two factors:  
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/06/06/71ExploreScheme_5/"/>
    <id>http://www.yulezhang.com/2021/06/06/71ExploreScheme_5/</id>
    <published>2021-06-06T14:05:00.000Z</published>
    <updated>2021-06-06T14:05:27.624Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;h2 id=&quot;本周工作&quot;&gt;本周工作&lt;/h2&gt;
&lt;p&gt;论文修改&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[x] introduction翻修&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[x] 添加PEPS的补充说明&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[x]
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/06/06/72ExploreScheme_6/"/>
    <id>http://www.yulezhang.com/2021/06/06/72ExploreScheme_6/</id>
    <published>2021-06-06T14:05:00.000Z</published>
    <updated>2021-06-14T03:29:15.883Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;之前一直不明白为什么程序里要每次保存模型，然后读取模型进行计算准确率。多此一举，为什么不让所有的操作都在内存中完成呢。今天我突然明白原因了，就是因为GTNC的参数太多，之前标签训练完的模型占据着大量的参数，如果不保存，不同标签的参数内存就会累加，可想而知，很容易溢出了。&lt;/
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/05/17/69ExploreScheme_3/"/>
    <id>http://www.yulezhang.com/2021/05/17/69ExploreScheme_3/</id>
    <published>2021-05-17T12:46:00.000Z</published>
    <updated>2021-07-11T11:34:35.265Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;h2 id=&quot;本周工作&quot;&gt;本周工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;继续搜索GTNC的参数&lt;strong&gt;map_dim,&lt;/strong&gt; 结果如图1所示，batch
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/05/17/70ExploreScheme_4/"/>
    <id>http://www.yulezhang.com/2021/05/17/70ExploreScheme_4/</id>
    <published>2021-05-17T12:46:00.000Z</published>
    <updated>2021-05-30T12:42:35.513Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;h2 id=&quot;本周工作&quot;&gt;本周工作&lt;/h2&gt;
&lt;h3
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>talks</title>
    <link href="http://www.yulezhang.com/2021/05/12/68ExploreScheme_2/"/>
    <id>http://www.yulezhang.com/2021/05/12/68ExploreScheme_2/</id>
    <published>2021-05-12T13:38:00.000Z</published>
    <updated>2021-05-19T13:28:46.318Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;h2
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Experiments</title>
    <link href="http://www.yulezhang.com/2021/05/06/67ExploreScheme_1/"/>
    <id>http://www.yulezhang.com/2021/05/06/67ExploreScheme_1/</id>
    <published>2021-05-06T10:52:00.000Z</published>
    <updated>2021-05-12T14:34:13.866Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;h2
        
      
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>TGen-NU model fusion</title>
    <link href="http://www.yulezhang.com/2021/05/02/66CodeDetect/"/>
    <id>http://www.yulezhang.com/2021/05/02/66CodeDetect/</id>
    <published>2021-05-02T12:46:00.000Z</published>
    <updated>2021-05-03T08:01:28.468Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本周工作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TGAN生成X-ray图片&lt;/li&gt;
&lt;li&gt;GenPU生成X-ray图片&lt;/li&gt;
&lt;li&gt;设计&lt;strong&gt;TGen-NU&lt;/strong&gt;模型，并尝试生成X-ray图片&lt;/li&gt;
&lt;li&gt;解决归一化问题&lt;sup&gt;[1]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;基本完成TGen-NU的生成任务，但仍存在&lt;strong&gt;不稳定、不收敛&lt;/strong&gt;问题&lt;sup&gt;[2]&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;查阅学习GAN的公式推导、扩展资料如&lt;strong&gt;WGAN、DCGAN&lt;/strong&gt;等&lt;/li&gt;
&lt;li&gt;新思路：把MPS融入到GenPU/TGen-NU模型中&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>TGAN and GenPU</title>
    <link href="http://www.yulezhang.com/2021/04/23/65TGAN-GenPU/"/>
    <id>http://www.yulezhang.com/2021/04/23/65TGAN-GenPU/</id>
    <published>2021-04-23T02:35:00.000Z</published>
    <updated>2021-04-25T11:26:16.123Z</updated>
    
    <summary type="html">
    
      &lt;h3 id=&quot;bilibili-robust-low-tubal-rank-tensor-recovery-from-binary-measurements&quot;&gt;【&lt;a href=&quot;https://www.bilibili.com/s/video/BV1QK4y1N7dp&quot;&gt;bilibili&lt;/a&gt;】&lt;strong&gt;Robust Low-tubal-rank Tensor Recovery from Binary Measurements&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在看这个视频时，顺带又回顾了一下张量秩的概念，目前并没有给出统一的很明确的定义，张量秩的定义往往跟分解算法有关&lt;/p&gt;
&lt;p&gt;在CP分解中，张量的秩定义如下&lt;/p&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>FCTN-Decomposition-Code</title>
    <link href="http://www.yulezhang.com/2021/04/18/64FCTN-Code-Analysis/"/>
    <id>http://www.yulezhang.com/2021/04/18/64FCTN-Code-Analysis/</id>
    <published>2021-04-18T12:59:00.000Z</published>
    <updated>2021-04-18T12:59:18.931Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本周主要是想抓紧把之前TTN和MPO作用batch的思路落实一下，并理解FCTN的matlab代码&lt;/p&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>FCTN-Decomposition</title>
    <link href="http://www.yulezhang.com/2021/04/11/63FCTN-Decomposition/"/>
    <id>http://www.yulezhang.com/2021/04/11/63FCTN-Decomposition/</id>
    <published>2021-04-11T02:41:00.000Z</published>
    <updated>2021-04-18T10:47:58.958Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本周花了大量时间精读FCTN，还联系上了原作者郑博士，弄懂了论文中的公式推到部分。 同时这次的学习也将之前忽略的很多细节也抓了起来，比如参数复杂度、计算复杂度等等，不提高自己对公式的理解是没法提高论文的档次的，这篇文章的公式恰好不是很难，便于入门。&lt;/p&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>量子机器学习资料整理</title>
    <link href="http://www.yulezhang.com/2021/03/28/62QML-Collections/"/>
    <id>http://www.yulezhang.com/2021/03/28/62QML-Collections/</id>
    <published>2021-03-28T12:50:00.000Z</published>
    <updated>2021-04-04T07:00:29.408Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;由于目前张量网络的工作逐渐增多，很多实验室都封装了自己的张量收缩算法包，同时还有一些论文给出了具体的工作，在此归类梳理一下方便日后的学习。&lt;/p&gt;
&lt;h2 id=&quot;workshop&quot;&gt;Workshop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://tensorworkshop.github.io/2020/index.html&quot;&gt;&lt;strong&gt;IJCAL&lt;/strong&gt;&lt;/a&gt;: International Workshop on Tensor Network Representations in Machine Learning&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tensorworkshop.github.io/NeurIPS2020/&quot;&gt;&lt;strong&gt;NeurIPS2020&lt;/strong&gt;&lt;/a&gt;: Quantum tensor networks in machine learning&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ipam.ucla.edu/wp-content/uploads/2019/09/TMWS2-Poster.pdf&quot;&gt;ipam&lt;/a&gt;: Tensor Network States and Applications, &lt;strong&gt;APRIL 19 - 23, 2021&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.yulezhang.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>周报-26</title>
    <link href="http://www.yulezhang.com/2021/03/27/60ReadBook/"/>
    <id>http://www.yulezhang.com/2021/03/27/60ReadBook/</id>
    <published>2021-03-27T13:15:14.000Z</published>
    <updated>2021-03-28T13:47:22.959Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;总结了一下华为软挑比赛，同时整理了一下&lt;a href=&quot;http://www.yulezhang.com/2021/03/28/62QML-Collections/&quot;&gt;量子机器学习的代码学习资料&lt;/a&gt;，并对二维MERA结构进行了数据集拓展、阅读《智能时代》。&lt;/p&gt;
    
    </summary>
    
    
      <category term="周汇报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%B1%87%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>华为软件精英挑战赛</title>
    <link href="http://www.yulezhang.com/2021/03/27/61HuaweiSoftwareContest/"/>
    <id>http://www.yulezhang.com/2021/03/27/61HuaweiSoftwareContest/</id>
    <published>2021-03-27T10:00:00.000Z</published>
    <updated>2021-03-28T12:58:12.507Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;能够承认自己的普通并非易事&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HHL and TEBD</title>
    <link href="http://www.yulezhang.com/2021/03/20/59CodeBreakthrough/"/>
    <id>http://www.yulezhang.com/2021/03/20/59CodeBreakthrough/</id>
    <published>2021-03-20T00:33:14.000Z</published>
    <updated>2021-03-21T05:42:33.443Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本周汇报目前将分为以下三个部分&lt;/p&gt;
&lt;h1&gt;量子算法回顾&lt;/h1&gt;
&lt;p&gt;在周二下午的汇报之后发现对量子&lt;strong&gt;傅里叶变换、量子相位估计算法、Shor以及HHL算法&lt;/strong&gt;理解的还不够透彻，下面列出了一些遗留的疑难问题的讨论和补充。汇报PPT点击&lt;a href=&quot;/download/QuantumAlgorithm/20190721-%E9%87%8F%E5%AD%90%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98-%E5%BC%A0%E5%AE%87.pptx&quot;&gt;此处&lt;/a&gt;下载&lt;/p&gt;
    
    </summary>
    
    
      <category term="周汇报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%B1%87%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Grover</title>
    <link href="http://www.yulezhang.com/2021/03/14/58Grover/"/>
    <id>http://www.yulezhang.com/2021/03/14/58Grover/</id>
    <published>2021-03-14T13:21:17.000Z</published>
    <updated>2021-03-14T14:39:16.425Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Grover算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;搜索算法是利用计算机的高性能来有目的的穷举一个问题解空间的部分或所有的可能情况，从而求出问题的解的一种方法。现阶段一般有枚举算法、深度优先搜索、广度优先搜索、A*算法、回溯算法、蒙特卡洛树搜索、散列函数等算法。比较常见的一种应用场景就是A到B的最短耗时路径搜索，一般的传统方法我们至少要把所有的路径遍历一遍来求解，而量子Grover算法只需要&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msqrt&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;/msqrt&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;O(\sqrt(n))&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.24em;vertical-align:-0.30499999999999994em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord sqrt&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.935em;&quot;&gt;&lt;span class=&quot;svg-align&quot; style=&quot;top:-3.2em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3.2em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot; style=&quot;padding-left:1em;&quot;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-2.8950000000000005em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3.2em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;hide-tail&quot; style=&quot;min-width:1.02em;height:1.28em;&quot;&gt;&lt;svg width=&#39;400em&#39; height=&#39;1.28em&#39; viewBox=&#39;0 0 400000 1296&#39; preserveAspectRatio=&#39;xMinYMin slice&#39;&gt;&lt;path d=&#39;M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z&#39;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.30499999999999994em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;的复杂度就能解决该问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="周汇报" scheme="http://www.yulezhang.com/categories/%E5%91%A8%E6%B1%87%E6%8A%A5/"/>
    
    
  </entry>
  
</feed>
